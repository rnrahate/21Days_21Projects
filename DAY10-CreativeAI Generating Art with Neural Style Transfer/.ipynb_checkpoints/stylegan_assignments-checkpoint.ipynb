{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f403461",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision pillow tqdm numpy matplotlib huggingface_hub\n",
    "!pip install git+https://github.com/NVlabs/stylegan2-ada-pytorch.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bbad5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "import dnnlib\n",
    "import legacy\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(\"Using device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cf479fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download pretrained StyleGAN2-FFHQ model from official NVlabs\n",
    "network_url = \"https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/ffhq.pkl\"\n",
    "\n",
    "with dnnlib.util.open_url(network_url) as f:\n",
    "    G = legacy.load_network_pkl(f)['G_ema'].to(device)  # type: ignore\n",
    "\n",
    "print(\"Loaded StyleGAN2-FFHQ generator\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c233412",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_image(G, w, truncation=0.7):\n",
    "    img = G.synthesis(w, noise_mode='const')\n",
    "    img = (img.clamp(-1,1) + 1) * 127.5\n",
    "    img = img.permute(0,2,3,1).cpu().numpy().astype(np.uint8)\n",
    "    return img[0]\n",
    "\n",
    "def show_images(imgs, titles=None, cols=5, size=3):\n",
    "    rows = (len(imgs)+cols-1)//cols\n",
    "    plt.figure(figsize=(cols*size, rows*size))\n",
    "    for i,img in enumerate(imgs):\n",
    "        plt.subplot(rows, cols, i+1)\n",
    "        plt.imshow(img)\n",
    "        plt.axis(\"off\")\n",
    "        if titles: plt.title(titles[i])\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6773f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_boundary = hf_hub_download(repo_id=\"yuval-alaluf/stylegan2-ffhq\", filename=\"boundaries/gender_boundary.npy\")\n",
    "age_boundary = hf_hub_download(repo_id=\"yuval-alaluf/stylegan2-ffhq\", filename=\"boundaries/age_boundary.npy\")\n",
    "\n",
    "gender_vec = np.load(gender_boundary)\n",
    "age_vec = np.load(age_boundary)\n",
    "\n",
    "print(\"Loaded gender and age boundaries\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d77a12d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 1: Gender manipulation\n",
    "z = torch.randn([1, G.z_dim], device=device)\n",
    "w = G.mapping(z, None)\n",
    "\n",
    "steps = np.linspace(-3, 3, 10)\n",
    "imgs = []\n",
    "for s in steps:\n",
    "    w_edit = w + torch.tensor(s * gender_vec, device=device).unsqueeze(0)\n",
    "    img = generate_image(G, w_edit)\n",
    "    imgs.append(img)\n",
    "\n",
    "show_images(imgs, titles=[f\"step {round(s,2)}\" for s in steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f19e582",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 2: Age manipulation\n",
    "z = torch.randn([1, G.z_dim], device=device)\n",
    "w = G.mapping(z, None)\n",
    "\n",
    "steps = np.linspace(-3, 3, 10)\n",
    "imgs = []\n",
    "for s in steps:\n",
    "    w_edit = w + torch.tensor(s * age_vec, device=device).unsqueeze(0)\n",
    "    img = generate_image(G, w_edit)\n",
    "    imgs.append(img)\n",
    "\n",
    "show_images(imgs, titles=[f\"age {round(s,2)}\" for s in steps])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf823818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assignment 3: Style mixing\n",
    "\n",
    "z1 = torch.randn([1, G.z_dim], device=device)\n",
    "z2 = torch.randn([1, G.z_dim], device=device)\n",
    "w1 = G.mapping(z1, None)\n",
    "w2 = G.mapping(z2, None)\n",
    "\n",
    "# Style mixing: use first 5 layers from w1, rest from w2\n",
    "w_mixed = w1.clone()\n",
    "w_mixed[:, 5:] = w2[:, 5:]\n",
    "\n",
    "img1 = generate_image(G, w1)\n",
    "img2 = generate_image(G, w2)\n",
    "img_mix = generate_image(G, w_mixed)\n",
    "\n",
    "show_images([img1, img2, img_mix], titles=[\"Image 1\", \"Image 2\", \"Mixed\"])\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
